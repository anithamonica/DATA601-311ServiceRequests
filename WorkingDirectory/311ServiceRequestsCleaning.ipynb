{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d7576b0-6845-4c55-8f7f-c507ce6d0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bee5d8bb-b32a-4b75-984d-54af7960bfdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service_request_id</th>\n",
       "      <th>requested_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>status_description</th>\n",
       "      <th>source</th>\n",
       "      <th>service_name</th>\n",
       "      <th>agency_responsible</th>\n",
       "      <th>address</th>\n",
       "      <th>comm_code</th>\n",
       "      <th>comm_name</th>\n",
       "      <th>location_type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23-00000797</td>\n",
       "      <td>2023/01/02 12:00:00 AM</td>\n",
       "      <td>2023/01/10 12:00:00 AM</td>\n",
       "      <td>2023/01/10 12:00:00 AM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finance - ONLINE TIPP Agreement Request</td>\n",
       "      <td>CFOD - Finance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23-00001045</td>\n",
       "      <td>2023/01/02 12:00:00 AM</td>\n",
       "      <td>2024/01/11 12:00:00 AM</td>\n",
       "      <td>2024/01/11 12:00:00 AM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Active Living Program Application</td>\n",
       "      <td>CS - Recreation and Social Programs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-00001163</td>\n",
       "      <td>2023/01/02 12:00:00 AM</td>\n",
       "      <td>2023/01/06 12:00:00 AM</td>\n",
       "      <td>2023/01/06 12:00:00 AM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Phone</td>\n",
       "      <td>CN - Registered Social Worker Letter</td>\n",
       "      <td>CS - Calgary Neighbourhoods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23-00001191</td>\n",
       "      <td>2023/01/02 12:00:00 AM</td>\n",
       "      <td>2024/05/19 12:00:00 AM</td>\n",
       "      <td>2023/01/10 12:00:00 AM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Other</td>\n",
       "      <td>CT - Lost Property</td>\n",
       "      <td>OS - Calgary Transit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-00001584</td>\n",
       "      <td>2023/01/02 12:00:00 AM</td>\n",
       "      <td>2023/01/04 12:00:00 AM</td>\n",
       "      <td>2023/01/04 12:00:00 AM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Other</td>\n",
       "      <td>Recreation - Arena Booking Application</td>\n",
       "      <td>CS - Calgary Recreation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  service_request_id          requested_date            updated_date  \\\n",
       "0        23-00000797  2023/01/02 12:00:00 AM  2023/01/10 12:00:00 AM   \n",
       "1        23-00001045  2023/01/02 12:00:00 AM  2024/01/11 12:00:00 AM   \n",
       "2        23-00001163  2023/01/02 12:00:00 AM  2023/01/06 12:00:00 AM   \n",
       "3        23-00001191  2023/01/02 12:00:00 AM  2024/05/19 12:00:00 AM   \n",
       "4        23-00001584  2023/01/02 12:00:00 AM  2023/01/04 12:00:00 AM   \n",
       "\n",
       "              closed_date status_description source  \\\n",
       "0  2023/01/10 12:00:00 AM             Closed  Other   \n",
       "1  2024/01/11 12:00:00 AM             Closed  Other   \n",
       "2  2023/01/06 12:00:00 AM             Closed  Phone   \n",
       "3  2023/01/10 12:00:00 AM             Closed  Other   \n",
       "4  2023/01/04 12:00:00 AM             Closed  Other   \n",
       "\n",
       "                              service_name  \\\n",
       "0  Finance - ONLINE TIPP Agreement Request   \n",
       "1        Active Living Program Application   \n",
       "2     CN - Registered Social Worker Letter   \n",
       "3                       CT - Lost Property   \n",
       "4   Recreation - Arena Booking Application   \n",
       "\n",
       "                    agency_responsible  address comm_code comm_name  \\\n",
       "0                       CFOD - Finance      NaN       NaN       NaN   \n",
       "1  CS - Recreation and Social Programs      NaN       NaN       NaN   \n",
       "2          CS - Calgary Neighbourhoods      NaN       NaN       NaN   \n",
       "3                 OS - Calgary Transit      NaN       NaN       NaN   \n",
       "4              CS - Calgary Recreation      NaN       NaN       NaN   \n",
       "\n",
       "  location_type  longitude  latitude point  \n",
       "0           NaN        NaN       NaN   NaN  \n",
       "1           NaN        NaN       NaN   NaN  \n",
       "2           NaN        NaN       NaN   NaN  \n",
       "3           NaN        NaN       NaN   NaN  \n",
       "4           NaN        NaN       NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "filename = \"311_Service_Requests_2yrs.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aa911be-4d79-427e-b05c-7db41c71c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning: Drop rows\n",
    "\n",
    "df = df.iloc[:1062842]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "234c63b8-3ea0-4506-b79f-6f62a07d120c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1062842, 15)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c828b65c-e2f6-41db-8982-6bd09c370276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['service_request_id', 'requested_date', 'updated_date', 'closed_date',\n",
       "       'status_description', 'source', 'service_name', 'agency_responsible',\n",
       "       'address', 'comm_code', 'comm_name', 'location_type', 'longitude',\n",
       "       'latitude', 'point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5008b84-ff33-43c3-b552-35c9423c7e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_request_id     object\n",
       "requested_date         object\n",
       "updated_date           object\n",
       "closed_date            object\n",
       "status_description     object\n",
       "source                 object\n",
       "service_name           object\n",
       "agency_responsible     object\n",
       "address               float64\n",
       "comm_code              object\n",
       "comm_name              object\n",
       "location_type          object\n",
       "longitude             float64\n",
       "latitude              float64\n",
       "point                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Datatypes\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc9b800-7892-4dbe-b531-5efc89377301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_request_id          0\n",
       "requested_date              0\n",
       "updated_date                0\n",
       "closed_date             32093\n",
       "status_description          0\n",
       "source                      0\n",
       "service_name                0\n",
       "agency_responsible        158\n",
       "address               1062842\n",
       "comm_code               73834\n",
       "comm_name               73833\n",
       "location_type           73626\n",
       "longitude               73845\n",
       "latitude                73845\n",
       "point                   73845\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying number of missing values \n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c530603a-3090-4a72-a7f7-b2a0a29998ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1062842, 14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping address column\n",
    "\n",
    "df = df.drop('address',axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db41908f-d636-4b8d-baa0-4148e8f6fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'requested_date': datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Deriving new columns from requested date\n",
    "\n",
    "df['requested_date'] = pd.to_datetime(df['requested_date'], format = '%Y/%m/%d %I:%M:%S %p')\n",
    "print(f\"Data type of 'requested_date': {df['requested_date'].dtype}\")\n",
    "\n",
    "df['request_year'] = df['requested_date'].dt.year\n",
    "df['request_month'] = df['requested_date'].dt.month\n",
    "df['request_day'] = df['requested_date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba4f0c1c-8fac-4a5d-8fd6-b3eb32509352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['updated_date'] = pd.to_datetime(df['updated_date'], format = '%Y/%m/%d %I:%M:%S %p')\n",
    "\n",
    "df['update_year'] = df['updated_date'].dt.year\n",
    "df['update_month'] = df['updated_date'].dt.month\n",
    "df['update_day'] = df['updated_date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "239b1425-73c1-4da8-a036-87afcdbe0fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Deriving new columns from requested date\n",
    "\n",
    "df['closed_date'] = pd.to_datetime(df['closed_date'], format = '%Y/%m/%d %I:%M:%S %p')\n",
    "print(\"Dataype:\", df['closed_date'].dtype)\n",
    "\n",
    "# Converting null values to NaT\n",
    "df['closed_date'] = df['closed_date'].fillna(pd.NaT)\n",
    "\n",
    "\n",
    "df['closed_year'] = df['closed_date'].dt.year\n",
    "df['closed_month'] = df['closed_date'].dt.month\n",
    "df['closed_day'] = df['closed_date'].dt.day\n",
    "\n",
    "# Replacing null values in derived columns with 0 and converting the column values to int type\n",
    "\n",
    "df.loc[df['closed_date'].isna(), ['closed_year', 'closed_month', 'closed_day']] = 0\n",
    "df[['closed_year', 'closed_month', 'closed_day']] = df[['closed_year', 'closed_month', 'closed_day']].astype('Int32')\n",
    "\n",
    "#display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b230d90-0f9c-4621-b598-bbee0ab71943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_request_id            object\n",
       "requested_date        datetime64[ns]\n",
       "updated_date          datetime64[ns]\n",
       "closed_date           datetime64[ns]\n",
       "status_description            object\n",
       "source                        object\n",
       "service_name                  object\n",
       "agency_responsible            object\n",
       "comm_code                     object\n",
       "comm_name                     object\n",
       "location_type                 object\n",
       "longitude                    float64\n",
       "latitude                     float64\n",
       "point                         object\n",
       "request_year                   int32\n",
       "request_month                  int32\n",
       "request_day                    int32\n",
       "update_year                    int32\n",
       "update_month                   int32\n",
       "update_day                     int32\n",
       "closed_year                    Int32\n",
       "closed_month                   Int32\n",
       "closed_day                     Int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking datatypes ofconverted columns\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fe45c54-dd18-4fae-9603-e2e91b0d85bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missingDataPercentage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Handling Missing Data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m columnNameDropped \u001b[38;5;241m=\u001b[39m missingDataPercentage[missingDataPercentage \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mColumns with missing percentage more than 40\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m missing values are:\u001b[39m\u001b[38;5;124m\"\u001b[39m, columnNameDropped)\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m missingDataPercentage[missingDataPercentage \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m40\u001b[39m]\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'missingDataPercentage' is not defined"
     ]
    }
   ],
   "source": [
    "# Handling Missing Data\n",
    "columnNameDropped = missingDataPercentage[missingDataPercentage >= 40].index.tolist()\n",
    "print(\"\\nColumns with missing percentage more than 40% missing values are:\", columnNameDropped)\n",
    "df = df.drop(columns = missingDataPercentage[missingDataPercentage > 40].index)\n",
    "\n",
    "# Handling Unwanted Data\n",
    "beforeCount = df.shape[0]\n",
    "df = df[(df['requested_date'] < '2025-01-01') & (df['requested_date'] > '2023-01-01')]\n",
    "afterCount =df.shape[0]\n",
    "deletedCount = beforeCount - afterCount\n",
    "print(f\"\\nCount of deleted request which are recieved on or after 2025-01-01 and before 2023-01-01: {deletedCount}\")\n",
    "\n",
    "#Handling Missing Community Code\n",
    "communityNames = df[df['comm_code'].isnull() & df['comm_name'].notnull()]['comm_name']\n",
    "print(f\"\\nCommunity name with community code null and community name exists: {communityNames}\")\n",
    "\n",
    "df['comm_code'].fillna(df['comm_name'], inplace=True)\n",
    "print(f\"\\nCommunity Code is filled with Community name for {communityNames} community\")\n",
    "\n",
    "#Handling Missing Longitude and Latitude with their median \n",
    "df['longitude'] = df['longitude'].fillna(df['longitude'].median())\n",
    "df['latitude'] = df['latitude'].fillna(df['latitude'].median())\n",
    "print(\"\\nLongitude and latitude missing values are replaced with its corresponding median\")\n",
    "\n",
    "#Handling Missing Point with the mode\n",
    "df['point'] = df['point'].fillna(df['point'].mode()[0])\n",
    "print(\"\\nPoint missing values are replaced with its mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679c889-0251-48d6-a3e2-09ae5b6913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating closing delay and creating new inttype column for closing delay\n",
    "\n",
    "df['response_time'] = df['closed_date'] - df['requested_date']\n",
    "print(\"1\",df['response_time'].dtype)\n",
    "df['response_time'] = df['response_time'].dt.days\n",
    "print(df['response_time'].dtype)\n",
    "df['response_time'] = df['response_time'].astype('Int64')\n",
    "\n",
    "print(df['response_time'].dtype)\n",
    "\n",
    "#Check\n",
    "#df_subset = df.iloc[1050:1116]  # Python slicing includes 150 but excludes 166\n",
    "#display(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5c86f-6893-4eb0-9cdf-927bf9bd03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate requests using regex and creating new column \n",
    "\n",
    "df['duplicate_request'] = df['status_description'].str.contains(r'Duplicate \\(Closed\\)', regex=True)\n",
    "\n",
    "# Convert the boolean values to 'Yes'/'No'\n",
    "df['duplicate_request'] = df['duplicate_request'].replace({True: 'Yes', False: 'No'})\n",
    "\n",
    "# Check\n",
    "#df_subset = df.iloc[150:166]  # Python slicing includes 150 but excludes 166\n",
    "#display(df_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea93dd7-187c-438c-8f75-d353e374048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max date value in requested_date column\n",
    "\n",
    "max_value = df['requested_date'].max()\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24876b2-a722-4af9-a1e6-1e0b0946ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season Categorisation of \"Requests\"\n",
    "\n",
    "# Defining Calgary's timezone\n",
    "calgary_tz = pytz.timezone('America/Edmonton')  \n",
    "\n",
    "# Exact UTC times for solstices and equinoxes (taken from Govt of Canada Website)\n",
    "\n",
    "seasons_utc = {\n",
    "    'Spring_2023': '2023-03-20 21:24:00',\n",
    "    'Summer_2023': '2023-06-21 14:57:00',\n",
    "    'Autumn_2023': '2023-09-23 06:50:00',\n",
    "    'Winter_2023': '2023-12-22 03:27:00',\n",
    "    'Spring_2024': '2024-03-20 03:06:00',\n",
    "    'Summer_2024': '2024-06-20 20:50:00',\n",
    "    'Autumn_2024': '2024-09-22 12:43:00',\n",
    "    'Winter_2024': '2024-12-21 09:20:00'\n",
    "}\n",
    "\n",
    "# Converting the UTC times to Calgary local time\n",
    "\n",
    "seasons = {}\n",
    "\n",
    "for season, utc_time_str in seasons_utc.items():\n",
    "    \n",
    "    # Converting the UTC string into a datetime object\n",
    "    \n",
    "    utc_time = datetime.strptime(utc_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    utc_time = pytz.utc.localize(utc_time) \n",
    "    \n",
    "    # Converting to Calgary local time\n",
    "    local_time = utc_time.astimezone(calgary_tz)\n",
    "    \n",
    "    # Saving the result in the dictionary\n",
    "    seasons[season] = local_time\n",
    "    \n",
    "for key, value in seasons.items():\n",
    "#print(f\"{key}: {value.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b3701-b920-4869-8903-b9c251b5d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keeping the local time but making it aware for requested_date columns\n",
    "\n",
    "if df['requested_date'].dt.tz is None:\n",
    "    df['new_requested_date'] = df['requested_date'].dt.tz_localize('America/Edmonton')\n",
    "\n",
    "print(df['new_requested_date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014e0b8-193c-4ec7-9c76-e0719d740ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing into seasons and creating a new 'season' column\n",
    "\n",
    "# Assigning seasons based on request date\n",
    "\n",
    "def get_season(request_date):\n",
    "    for season, season_date in seasons.items():\n",
    "        if request_date < season_date:\n",
    "            return season\n",
    "    return 'Winter_2024'  # Default to the latest season\n",
    "\n",
    "# Creating new season column \n",
    "\n",
    "df['Season'] = df['new_requested_date'].apply(get_season)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57462032-cbda-4e53-b61b-0c76aa64ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed requests taken into account\n",
    "\n",
    "statistics_response_time = df['response_time'].describe()\n",
    "print(statistics_response_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d2c3b-72a1-46c7-a7be-5db1b2fa01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking no of empty values in 'response_time' column\n",
    "\n",
    "na_count = df['response_time'].isna().sum()\n",
    "print(na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80c4e9-6e66-4529-96c0-44b03fb0485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of open requests \n",
    "\n",
    "no_open_requests = len(df[df['status_description'] == 'Open'])\n",
    "no_open_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86a2ee-ec8b-4ebb-a41c-e925f9338273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding data discrepencies (688 numbers because the missing values in closed_dates was found to be 32093)\n",
    "\n",
    "filtered_df = df[df['status_description'] == 'Open' ]\n",
    "grouped_data = filtered_df.groupby('status_description')['closed_date']\n",
    "pd.set_option('display.max_rows', None)\n",
    "#display(grouped_data.tail(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e9966-929d-4ee2-8d82-1add613543cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To solve this data discrepency, change the \"status\" of requests with closed dates to \"Closed\".\n",
    "\n",
    "df['modified_status'] = df.apply(\n",
    "   lambda row: 'Closed' if pd.notna(row['closed_date']) and row['status_description'] == 'Open' \n",
    "    else ('Duplicate (Closed)' if pd.notna(row['closed_date']) and row['status_description'] == 'Duplicate (Open)' \n",
    "          else row['status_description']), axis=1\n",
    ")\n",
    "display(df.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35839cb6-2d6f-4e26-b707-d82cfedaae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df['status_description'].value_counts()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443873ff-bed5-415c-a887-392d97064d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary1 = df['modified_status'].value_counts()\n",
    "print(summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e0476-eb48-4079-aded-615ca5543814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column for Community Sector using the community sector csv file\n",
    "community_data=pd.read_csv(\"/Users/jincythomas/Desktop/PROJECT-DATA601/311ServiceRequests/CSV_SECTORS.csv\")\n",
    "def merge_community_sector(main_data, community_data):\n",
    "    # Rename the relevant columns in the community_data for clarity and consistency\n",
    "    community_data.rename(columns={'COMM_CODE': 'comm_code', 'SECTOR': 'community_sector'}, inplace=True)\n",
    "\n",
    "    # Merge the datasets based on the 'comm_code'\n",
    "    merged_data = main_data.merge(community_data[['comm_code', 'community_sector']], on='comm_code', how='left')\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "df = merge_community_sector(df, community_data)\n",
    "print(\"\\n\\033[1m\"+\"Additional Columns created are:\"+\"\\033[0m\")\n",
    "print(\"\\tcommunity_sector\")\n",
    "\n",
    "#Handling Missing for Community related columns\n",
    "df['comm_code'].fillna(\"Community Centrepoint\", inplace=True)\n",
    "df['comm_name'].fillna(\"Community Centrepoint\", inplace=True)\n",
    "df['community_sector'].fillna(\"Community Centrepoint\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c658e1-32c0-4d92-8cba-88cc9016c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for Divisions of Agency assigned for the requests\n",
    "\n",
    "#Unassigned agencies are assigned to corresponding divisions\n",
    "df.loc[df['agency_responsible'].isnull() & df['service_name'].str.contains('WATR -'), 'agency_responsible'] = 'UEP - Utilities & Environmental Protection'\n",
    "df.loc[df['agency_responsible'].isnull() & df['service_name'].str.contains('PSD -'), 'agency_responsible'] = 'PDS - Planning & Development Services'\n",
    "df.loc[df['agency_responsible'].isnull() & df['service_name'].str.contains('CPI -'), 'agency_responsible'] = 'OSC - Operational Services and Compliance'\n",
    "\n",
    "# agency abbreviations are extracted\n",
    "def extract_division(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    parts = value.split('-')\n",
    "    resultStr = parts[0].strip() if '-' in value else value.strip()\n",
    "    return resultStr\n",
    "\n",
    "\n",
    "df['agency_division'] = df['agency_responsible'].apply(extract_division)\n",
    "#Actual agencies or divisions under Calgary Government\n",
    "agency_division = {\n",
    "    'agency_name': ['Affiliated Organizations', 'Chief Financial Officer Department', 'Corporate Wide Service Requests',\n",
    "                    'Calgary Police & Fire Services', 'Community Services', \"Deputy City Manager's Office\",\n",
    "                   'Elected Officials', 'Fleet and Inventory', 'Information Services','Legal or Legislative Services',\n",
    "                   'Office of the City Auditor','Operational Services and Compliance', 'Partnerships',\n",
    "                   'Planning & Development Services','Project Information and Control Systems', 'Recreation and Social Programs',\n",
    "                    'Transportation', 'Utilities & Environmental Protection'],\n",
    "    'abbreviations': [['AO', 'Affiliated Organizations'], ['CFOD'], ['Corporate Wide Service Requests'], \n",
    "                      ['CPFS'],['CS'], ['DCMO'], \n",
    "                      ['Elected Officials'], ['Fleet and Inventory'], ['IS'], ['LL','LLSS'],\n",
    "                      ['Office of the City Auditor'],['OS','OSC'],['Partnerships'],\n",
    "                      ['PD','PDS'],['PICS'],['Recreation and Social Programs'],\n",
    "                      ['TRAN','Tranc'], ['UEP','Uepc']]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a mapping dictionary\n",
    "mapping = {abbreviation: agency_name \n",
    "           for agency_name, abbreviations in zip(agency_division['agency_name'], agency_division['abbreviations']) \n",
    "           for abbreviation in abbreviations}\n",
    "\n",
    "\n",
    "# Replace the agency_division values with actual agency_name or divisions\n",
    "df['agency_division'] = df['agency_division'].map(mapping)\n",
    "\n",
    "#noDivisionDF = df[df['agency_division'].isnull()]\n",
    "#display(noDivisionDF)\n",
    "\n",
    "agencies= df['agency_division'].unique()\n",
    "    \n",
    "# Iterate through each agency division in the list\n",
    "for division in agencies:\n",
    "    subset_df = df[df['agency_division'] == division]\n",
    "    \n",
    "    # Split the 'agency_responsible' column at the first hyphen and create 'agency_subdivision'\n",
    "    df.loc[df['agency_division'] == division, 'agency_subdivision'] = subset_df['agency_responsible'].apply(\n",
    "        lambda x: x.split('-', 1)[1] if '-' in x else division\n",
    "    )\n",
    "\n",
    "    # Split the 'service_name' column at the first hyphen and create 'service_category'\n",
    "    df.loc[df['agency_division'] == division, 'service_category'] = subset_df['service_name'].apply(\n",
    "        lambda x: x.split('-', 1)[0] if '-' in x else x\n",
    "    )\n",
    "\n",
    "    # Split the 'service_name' column at the first hyphen and create 'service_request'\n",
    "    df.loc[df['agency_division'] == division, 'service_request'] = subset_df['service_name'].apply(\n",
    "        lambda x: x.split('-', 1)[1] if '-' in x else x\n",
    "    )\n",
    "    \n",
    "# Display the updated DataFrame\n",
    "#print(\"Updated DataFrame:\")\n",
    "display(df.head(100))\n",
    "\n",
    "\n",
    "print(\"\\n\\033[1m\"+\"Additional Columns created are:\"+\"\\033[0m\")\n",
    "print(\"\\tagency_division\")\n",
    "print(\"\\tagency_subdivision\")\n",
    "print(\"\\tservice_category\")\n",
    "print(\"\\tservice_request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03b20b-ebfb-41a8-9046-abbe81cd845f",
   "metadata": {},
   "source": [
    "## Response Efficiency\n",
    "\n",
    "To answer this we have to consider the requests which are not duplicate, and which has closed date>= requested_date\n",
    "\n",
    "### Questions?\n",
    "\n",
    "• Which agency handles the most and least number of service requests?\n",
    "\n",
    "• What is the average response rate and time for resolving for service requests for each agency?\n",
    "\n",
    "• Who are the most efficient agencies in terms of response and resolution times?\n",
    "\n",
    "• How does the response efficiency vary across different years?\n",
    "\n",
    "• How does the response efficiency vary across different years for each agency divisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1486c08e-96e2-4c96-a0a4-90bd7bafecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAgency division and the count of requests handles by each division:\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'duplicate_request'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'duplicate_request'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Filter the records from your dataframe df where closed_date is greater than or equal to requested_date, \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#closed_date is not null, and duplicate_request is 'No'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgency division and the count of requests handles by each division:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m efficiencyDF \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequested_date\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m      6\u001b[0m                  (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()) \u001b[38;5;241m&\u001b[39m \n\u001b[0;32m----> 7\u001b[0m                  (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduplicate_request\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor answering the response efficiency, we have considered \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mefficiencyDF\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requests\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#The most and least request handled agencies\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'duplicate_request'"
     ]
    }
   ],
   "source": [
    "#Filter the records from your dataframe df where closed_date is greater than or equal to requested_date, \n",
    "#closed_date is not null, and duplicate_request is 'No'\n",
    "\n",
    "print(\"\\n\\033[1m\"+\"Agency division and the count of requests handles by each division:\"+\"\\033[0m\")\n",
    "efficiencyDF = df[(df['closed_date'] >= df['requested_date']) &\n",
    "                 (df['closed_date'].notna()) & \n",
    "                 (df['duplicate_request'] == 'No')]\n",
    "print(f\"For answering the response efficiency, we have considered {efficiencyDF.shape[0]} requests\")\n",
    "\n",
    "#The most and least request handled agencies\n",
    "grouped_counts = efficiencyDF.groupby(['agency_division']).size().reset_index(name='count')\n",
    "sorted_counts = grouped_counts.sort_values(by='count', ascending=False)\n",
    "first_row = sorted_counts.head(1).to_string(index=False, header=False)\n",
    "last_row = sorted_counts.tail(1).to_string(index=False, header=False)\n",
    "print(f\"The agency that handled the highest number of service requests and its count: {first_row}\")\n",
    "print(f\"The agency that handled the lowest number of service requests and its count: {last_row}\")\n",
    "print(\"Agency division and the count of requests handles by each division is as given below:\")\n",
    "display(sorted_counts.head(6))\n",
    "\n",
    "# Group by 'agency_division' and calculate the count of requests and average response time\n",
    "print(\"\\n\\033[1m\"+\"Agency division, Count of requests and its efficiency:\"+\"\\033[0m\")\n",
    "groupedEfficiencyDF = efficiencyDF.groupby('agency_division').agg(\n",
    "    request_count=('service_request_id', 'size'),\n",
    "    average_response_time=('response_time', 'mean')\n",
    ").reset_index()\n",
    "groupedEfficiencyDF['average_response_time'] = groupedEfficiencyDF['average_response_time'].round(2)\n",
    "groupedEfficiencyDF = groupedEfficiencyDF.sort_values(by='average_response_time', ascending=True)\n",
    "first_row = groupedEfficiencyDF.head(1).to_string(index=False, header=False)\n",
    "last_row = groupedEfficiencyDF.tail(1).to_string(index=False, header=False)\n",
    "print(f\"The most efficient agency and its response time: {first_row}\")\n",
    "print(f\"The least efficient agency and its response time: {last_row}\")\n",
    "print(\"Agency division, Count of requests and its efficiency in days:\")\n",
    "display(groupedEfficiencyDF)\n",
    "\n",
    "\n",
    "# How does the response efficiency vary across different years\n",
    "print(\"\\n\\033[1m\"+\"Response efficiency of Agency division over years:\"+\"\\033[0m\")\n",
    "average_response_time_per_year = efficiencyDF.groupby('request_year').agg(\n",
    "    request_count=('service_request_id', 'size'),\n",
    "    average_response_time=('response_time', 'mean')\n",
    ").reset_index()\n",
    "average_response_time_per_year['average_response_time'] = average_response_time_per_year['average_response_time'].round(2)\n",
    "average_response_time_per_year = average_response_time_per_year.sort_values(by='request_year', ascending=True)\n",
    "print(\"Average response efficiency per year for all agency divisions:\")\n",
    "display(average_response_time_per_year)\n",
    "\n",
    "\n",
    "# How the response efficiency vary across different years for each agency divisions\n",
    "average_response_time_per_year_and_agency = efficiencyDF.groupby(['request_year', 'agency_division']).agg(\n",
    "    request_count=('service_request_id', 'size'),\n",
    "    average_response_time=('response_time', 'mean')\n",
    ").reset_index()\n",
    "average_response_time_per_year_and_agency['average_response_time'] = average_response_time_per_year_and_agency['average_response_time'].round(2)\n",
    "average_response_time_per_year_and_agency = average_response_time_per_year_and_agency.sort_values(by=['request_year', 'agency_division'], ascending=True)\n",
    "#display(average_response_time_per_year_and_agency)\n",
    "\n",
    "\n",
    "pivoted_df = average_response_time_per_year_and_agency.pivot(index='agency_division', columns='request_year', values=['request_count', 'average_response_time'])\n",
    "display(pivoted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a752b-50f4-46e7-b47e-c6aa84213867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
